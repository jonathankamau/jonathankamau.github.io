{
    "education-description-1": "In this Nanodegree, I covered the components of data streaming systems, ingesting data in real-time using Apache Kafka and Spark and running analysis. I made use of Use the Faust Stream Processing Python library to build a real-time stream-based application. I utilized the Confluent Kafka Python library for simple topic management, production, and consumption. I also learned the components of Spark Streaming (architecture and API), integrating Apache Spark Structured Streaming and Apache Kafka, manipulate data using Spark and understand the statistical report generated by the Structured Streaming console.",
    "education-description-2": "From the nanodegree I learnt how to Create user-friendly relational and NoSQL data models, create scalable and efficient data warehouses, Identify the appropriate use cases for different big data technologies, Work efficiently with massive datasets, Build and interact with a cloud-based data lake and Automate and monitor data pipelines. I developed proficiency in Spark, Airflow, and AWS tools."
}